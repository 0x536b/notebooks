{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd5589fa-5e87-4ca5-9b17-4f7ae0addfb9",
   "metadata": {},
   "source": [
    "## Reading/Processing files of a trained dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad88ad8-c435-4cb1-87df-b74097e33cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON = /home/suresh/2024/gs/datasets/models/truck/cameras.json\n",
      "PLY = /home/suresh/2024/gs/datasets/models/truck/point_cloud/iteration_30000/point_cloud.ply\n",
      "Device running on = cuda\n"
     ]
    }
   ],
   "source": [
    "# Basic imports and paths\n",
    "from plyfile import PlyData, PlyElement\n",
    "import json\n",
    "import torch\n",
    "from gs_lib import quaternion_to_rot, strip_symmetric, focal2fov, fov2focal, getWorld2View, getProjectionMatrix, scale_res, get_padded_size\n",
    "\n",
    "\n",
    "# The only two files that are needed for rendering.\n",
    "dataset = f\"/home/suresh/2024/gs/datasets/models/truck\"\n",
    "iteration = \"30000\"\n",
    "max_sh_degree = 3\n",
    "json_file = f\"{dataset}/cameras.json\"\n",
    "ply_file = f\"{dataset}/point_cloud/iteration_{iteration}/point_cloud.ply\"\n",
    "\n",
    "# Print to see if proper\n",
    "print(f\"JSON = {json_file}\")\n",
    "print(f\"PLY = {ply_file}\")\n",
    "\n",
    "# Check for the available paraller device\n",
    "device = \"mps\" if getattr(torch,'has_mps',False) else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device running on = {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b2d638-a0a4-4543-8b63-7fdc8335171e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PLY File stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c1b3ddb-55e4-4531-906b-b4c932693862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The structure of the ply file:\n",
      "------------------------------\n",
      "element vertex 2541226\n",
      "property float x\n",
      "property float y\n",
      "property float z\n",
      "property float nx\n",
      "property float ny\n",
      "property float nz\n",
      "property float f_dc_0\n",
      "property float f_dc_1\n",
      "property float f_dc_2\n",
      "property float f_rest_0\n",
      "property float f_rest_1\n",
      "property float f_rest_2\n",
      "property float f_rest_3\n",
      "property float f_rest_4\n",
      "property float f_rest_5\n",
      "property float f_rest_6\n",
      "property float f_rest_7\n",
      "property float f_rest_8\n",
      "property float f_rest_9\n",
      "property float f_rest_10\n",
      "property float f_rest_11\n",
      "property float f_rest_12\n",
      "property float f_rest_13\n",
      "property float f_rest_14\n",
      "property float f_rest_15\n",
      "property float f_rest_16\n",
      "property float f_rest_17\n",
      "property float f_rest_18\n",
      "property float f_rest_19\n",
      "property float f_rest_20\n",
      "property float f_rest_21\n",
      "property float f_rest_22\n",
      "property float f_rest_23\n",
      "property float f_rest_24\n",
      "property float f_rest_25\n",
      "property float f_rest_26\n",
      "property float f_rest_27\n",
      "property float f_rest_28\n",
      "property float f_rest_29\n",
      "property float f_rest_30\n",
      "property float f_rest_31\n",
      "property float f_rest_32\n",
      "property float f_rest_33\n",
      "property float f_rest_34\n",
      "property float f_rest_35\n",
      "property float f_rest_36\n",
      "property float f_rest_37\n",
      "property float f_rest_38\n",
      "property float f_rest_39\n",
      "property float f_rest_40\n",
      "property float f_rest_41\n",
      "property float f_rest_42\n",
      "property float f_rest_43\n",
      "property float f_rest_44\n",
      "property float opacity\n",
      "property float scale_0\n",
      "property float scale_1\n",
      "property float scale_2\n",
      "property float rot_0\n",
      "property float rot_1\n",
      "property float rot_2\n",
      "property float rot_3\n"
     ]
    }
   ],
   "source": [
    "# Readind the ply file.\n",
    "plydata = PlyData.read(ply_file)\n",
    "print(\"The structure of the ply file:\")\n",
    "print(\"------------------------------\")\n",
    "print(plydata.elements[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df0c7cff-124e-4146-a220-ff9a8951fe6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.memmap'>\n",
      "XYZ = torch.Size([2541226, 3]), e.g. xyz[0] = tensor([-0.1773,  1.4565, -0.6173])\n"
     ]
    }
   ],
   "source": [
    "# extract the XYZ coordinate of gaussians\n",
    "print(type(plydata.elements[0][\"x\"]))\n",
    "x = torch.tensor(plydata.elements[0][\"x\"])\n",
    "y = torch.tensor(plydata.elements[0][\"y\"])\n",
    "z = torch.tensor(plydata.elements[0][\"z\"])\n",
    "xyz = torch.stack((x,y,z), dim=1)\n",
    "print(f\"XYZ = {xyz.shape}, e.g. xyz[0] = {xyz[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15c03a8-e42d-4d51-bec0-faff983e0044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op = torch.Size([2541226])\n"
     ]
    }
   ],
   "source": [
    "# extract the opacity of gaussians\n",
    "op = torch.tensor(plydata.elements[0][\"opacity\"])\n",
    "print(f\"op = {op.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be866f31-bf15-4456-8919-32ae42529be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sh_dc = torch.Size([2541226, 3, 1])\n",
      "sh_dc = tensor([[-0.3723],\n",
      "        [-0.4185],\n",
      "        [-0.2502]])\n"
     ]
    }
   ],
   "source": [
    "# extract spherical harmonics, DC component of gaussians\n",
    "sh_dc = torch.zeros((xyz.shape[0], 3, 1))\n",
    "sh_dc[:, 0, 0] = torch.tensor(plydata.elements[0][\"f_dc_0\"])\n",
    "sh_dc[:, 1, 0] = torch.tensor(plydata.elements[0][\"f_dc_1\"])\n",
    "sh_dc[:, 2, 0] = torch.tensor(plydata.elements[0][\"f_dc_2\"])\n",
    "print(f\"sh_dc = {sh_dc.shape}\")\n",
    "print(f\"sh_dc = {sh_dc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "337ddb9e-9062-44fb-ba30-a0bb5c4f7af8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PlyProperty('x', 'float'), PlyProperty('y', 'float'), PlyProperty('z', 'float'), PlyProperty('nx', 'float'), PlyProperty('ny', 'float'), PlyProperty('nz', 'float'), PlyProperty('f_dc_0', 'float'), PlyProperty('f_dc_1', 'float'), PlyProperty('f_dc_2', 'float'), PlyProperty('f_rest_0', 'float'), PlyProperty('f_rest_1', 'float'), PlyProperty('f_rest_2', 'float'), PlyProperty('f_rest_3', 'float'), PlyProperty('f_rest_4', 'float'), PlyProperty('f_rest_5', 'float'), PlyProperty('f_rest_6', 'float'), PlyProperty('f_rest_7', 'float'), PlyProperty('f_rest_8', 'float'), PlyProperty('f_rest_9', 'float'), PlyProperty('f_rest_10', 'float'), PlyProperty('f_rest_11', 'float'), PlyProperty('f_rest_12', 'float'), PlyProperty('f_rest_13', 'float'), PlyProperty('f_rest_14', 'float'), PlyProperty('f_rest_15', 'float'), PlyProperty('f_rest_16', 'float'), PlyProperty('f_rest_17', 'float'), PlyProperty('f_rest_18', 'float'), PlyProperty('f_rest_19', 'float'), PlyProperty('f_rest_20', 'float'), PlyProperty('f_rest_21', 'float'), PlyProperty('f_rest_22', 'float'), PlyProperty('f_rest_23', 'float'), PlyProperty('f_rest_24', 'float'), PlyProperty('f_rest_25', 'float'), PlyProperty('f_rest_26', 'float'), PlyProperty('f_rest_27', 'float'), PlyProperty('f_rest_28', 'float'), PlyProperty('f_rest_29', 'float'), PlyProperty('f_rest_30', 'float'), PlyProperty('f_rest_31', 'float'), PlyProperty('f_rest_32', 'float'), PlyProperty('f_rest_33', 'float'), PlyProperty('f_rest_34', 'float'), PlyProperty('f_rest_35', 'float'), PlyProperty('f_rest_36', 'float'), PlyProperty('f_rest_37', 'float'), PlyProperty('f_rest_38', 'float'), PlyProperty('f_rest_39', 'float'), PlyProperty('f_rest_40', 'float'), PlyProperty('f_rest_41', 'float'), PlyProperty('f_rest_42', 'float'), PlyProperty('f_rest_43', 'float'), PlyProperty('f_rest_44', 'float'), PlyProperty('opacity', 'float'), PlyProperty('scale_0', 'float'), PlyProperty('scale_1', 'float'), PlyProperty('scale_2', 'float'), PlyProperty('rot_0', 'float'), PlyProperty('rot_1', 'float'), PlyProperty('rot_2', 'float'), PlyProperty('rot_3', 'float'))\n"
     ]
    }
   ],
   "source": [
    "# These are all the properties of a single vertex available in the ply file\n",
    "print(f\"{plydata.elements[0].properties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfd25625-2baf-4093-ae5c-2296059fea09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sh_extra = torch.Size([2541226, 3, 15])\n"
     ]
    }
   ],
   "source": [
    "# extract rest of the SH components of gaussians\n",
    "# First we exract all the available SH componenets that are under the name \"f_rest_<n>\"\n",
    "# We extract the string of the property and sort it. Then we extract the floating values of those properties\n",
    "# into a tensor. We can take only the amount we need. \n",
    "# Maximum of 3 degree SH is used in all implementations and training\n",
    "# 3degree SH = 3+3*15 components per degree, hence 3(f_dc_0 to f_dc_2) + 45(f_rest_0 to f_rest_44)\n",
    "\n",
    "extra_f_names = [p.name for p in plydata.elements[0].properties if p.name.startswith(\"f_rest_\")]\n",
    "extra_f_names = sorted(extra_f_names, key = lambda x: int(x.split('_')[-1]))\n",
    "assert len(extra_f_names) == 3*(max_sh_degree + 1) ** 2 - 3\n",
    "\n",
    "sh_extra = torch.zeros((xyz.shape[0], len(extra_f_names)))\n",
    "for idx, attr_name in enumerate(extra_f_names):\n",
    "    sh_extra[:, idx] = torch.tensor(plydata.elements[0][attr_name])\n",
    "sh_extra = sh_extra.reshape((sh_extra.shape[0], 3, (max_sh_degree + 1) ** 2 - 1))\n",
    "\n",
    "print(f\"sh_extra = {sh_extra.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "076c9b4b-f13b-4784-8909-9d87aa6c27f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scales = torch.Size([2541226, 3])\n",
      "Rotations = torch.Size([2541226, 4])\n"
     ]
    }
   ],
   "source": [
    "# extract the scales and rotations of gaussians\n",
    "# Similar to SH_rest, this is also done.\n",
    "scale_names = [p.name for p in plydata.elements[0].properties if p.name.startswith(\"scale_\")]\n",
    "scale_names = sorted(scale_names, key = lambda x: int(x.split('_')[-1]))\n",
    "scales = torch.zeros((xyz.shape[0], len(scale_names)))\n",
    "for idx, attr_name in enumerate(scale_names):\n",
    "    scales[:, idx] = torch.tensor(plydata.elements[0][attr_name])\n",
    "\n",
    "rot_names = [p.name for p in plydata.elements[0].properties if p.name.startswith(\"rot\")]\n",
    "rot_names = sorted(rot_names, key = lambda x: int(x.split('_')[-1]))\n",
    "rots = torch.zeros((xyz.shape[0], len(rot_names)))\n",
    "for idx, attr_name in enumerate(rot_names):\n",
    "    rots[:, idx] = torch.tensor(plydata.elements[0][attr_name])\n",
    "    \n",
    "print(f\"Scales = {scales.shape}\")\n",
    "print(f\"Rotations = {rots.shape}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feb6034c-6f0d-4c9c-a4ad-f92c5ebe0c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The scales, rotations and sh components are processed by some activation.\n",
    "# Right now, not really sure why. Probably has to do with backpropagation during training using pytorch\n",
    "\n",
    "\n",
    "# Scales are passed thru exp()\n",
    "# rotations are normalized thru torch.nn.functional.normalize()\n",
    "# opacities are passed thru sigmoid\n",
    "#\n",
    "# https://github.com/graphdeco-inria/gaussian-splatting/blob/d9fad7b3450bf4bd29316315032d57157e23a515/scene/gaussian_model.py#L33\n",
    "scales = torch.exp(scales)\n",
    "rots = torch.nn.functional.normalize(rots)\n",
    "op = torch.sigmoid(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f740c19-4505-476a-9ed9-ded94d007b81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH components = torch.Size([2541226, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "# Building a single tensor for all SH features\n",
    "sh_all = torch.cat([sh_dc, sh_extra], dim=2)\n",
    "print(f\"SH components = {sh_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82f830aa-440d-4e49-82c5-76baf5767159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we build all the other required components for GS rendering.\n",
    "# Refer to the gs_lib.py file for more info of the functions used. \n",
    "# They are defined in the order of appreance in the notebook.\n",
    "\n",
    "# First we need calculate the 3D covariance sigma(E).\n",
    "# This is the equation 6 in the official paper\n",
    "# E = R * S * S^T * R^T\n",
    "# \"*\" measn matrixmult, \"^T\" means transpose\n",
    "# S = Scaling matrix, R = Rotation Matrix\n",
    "\n",
    "# First lets build the rotation matrix using quaternions\n",
    "# https://www.songho.ca/math/quaternion/quaternion.html\n",
    "# The above link gives a great derivation\n",
    "# https://www.songho.ca/opengl/gl_quaternion.html - This formula is used in GS\n",
    "rots = quaternion_to_rot(rots)\n",
    "\n",
    "# Convert 3-values array to diagonal vector for scale matrix\n",
    "Scales = torch.zeros((scales.shape[0], 3, 3), dtype=torch.float32)\n",
    "Scales[:, 0, 0] = scales[:, 0]\n",
    "Scales[:, 1, 1] = scales[:, 1]\n",
    "Scales[:, 2, 2] = scales[:, 2]\n",
    "\n",
    "# now calculate sigma  E = R * S * S^T * R^T\n",
    "E_raw = rots @ Scales\n",
    "E_raw = E_raw @ E_raw.transpose(2, 1)\n",
    "\n",
    "\n",
    "# Now E is our covariance matrix.\n",
    "# Covariance matrices are symmetrical across the diagonal.\n",
    "# Hence, I think thats why the official paper strips the bottom \n",
    "# elements from the diagonal.\n",
    "# https://github.com/graphdeco-inria/gaussian-splatting/blob/d9fad7b3450bf4bd29316315032d57157e23a515/utils/general_utils.py#L64\n",
    "E = strip_symmetric(E_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a3bc577-ffe3-4504-a53f-e4ef73034ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del Scales, rot_names, scale_names, scales, rots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9061df8d-a4f8-4ac4-9440-0132a02788a4",
   "metadata": {},
   "source": [
    "## JSON File stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1f1d40d-dccd-4bf5-96ae-06c9215466d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images = 251\n",
      "Tile size \t\t\t= 16\n",
      "Total blocks \t\t\t= 2170\n",
      "Original Width x Height \t= 1957 x 1091\n",
      "Block-adjusted Width x Height \t= 992 x 560\n",
      "Focal_x \t\t\t= 581.3302001953125\n",
      "Focal_y \t\t\t= 578.6701049804688\n",
      "FOV_x \t\t\t\t= 1.3986958265304565\n",
      "FOV_y \t\t\t\t= 0.8816215991973877\n"
     ]
    }
   ],
   "source": [
    "# Each image used for training has its own parameters.\n",
    "# We can use the Camera intrinsic and extrinsic parameters used by the training images to render\n",
    "# In other words, lets render a image of the same size and resolution as the training\n",
    "with open(json_file) as f:\n",
    "    json_raw = f.read()\n",
    "json_parsed = json.loads(json_raw)\n",
    "print(f\"Images = {len(json_parsed)}\")\n",
    "\n",
    "img = 52  # choose a random image\n",
    "camera = json_parsed[img]  # has height, width, position, rotation, focal_x, focal_y\n",
    "# print(f\"{camera}\")\n",
    "w = camera['width']  # one value\n",
    "h = camera['height'] # one value\n",
    "focal_x = camera['fx']  # one value\n",
    "focal_y = camera['fy']  # one value\n",
    "cam_rotation = torch.tensor(camera['rotation'])  # 3x3 matrix\n",
    "cam_position = torch.tensor(camera['position'])  # 3-value vector\n",
    "\n",
    "fov_x = focal2fov(focal_x, w)\n",
    "fov_y = focal2fov(focal_y, h)\n",
    "\n",
    "# Set a z axis(depth) boundry. So gaussian before and after these bounds will be removed.\n",
    "# These are called clipping planes\n",
    "znear = 0.1\n",
    "zfar = 100.0 \n",
    "\n",
    "\n",
    "# Scaling down and block adjusting image sizes\n",
    "# We are doing this so that our output image size is divisible equally by the block size.\n",
    "# Also only focal lenght is dependent on the final image size. FOV remains the same.\n",
    "new_w, new_h = scale_res(w, h, 2)\n",
    "TILE_SIZE  = 16\n",
    "final_w, final_h, total_blocks = get_padded_size(new_w, new_h, TILE_SIZE)\n",
    "focal_x = fov2focal(fov_x, new_w)\n",
    "focal_y = fov2focal(fov_y, new_h)\n",
    "\n",
    "print(f\"Tile size \\t\\t\\t= {TILE_SIZE}\")\n",
    "print(f\"Total blocks \\t\\t\\t= {total_blocks}\")\n",
    "print(f\"Original Width x Height \\t= {w} x {h}\")\n",
    "print(f\"Block-adjusted Width x Height \\t= {final_w} x {final_h}\")\n",
    "print(f\"Focal_x \\t\\t\\t= {focal_x}\")\n",
    "print(f\"Focal_y \\t\\t\\t= {focal_y}\")\n",
    "print(f\"FOV_x \\t\\t\\t\\t= {fov_x}\")\n",
    "print(f\"FOV_y \\t\\t\\t\\t= {fov_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d251dda-d138-45f2-97ad-a668258663d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full projection matrix : tensor([[ 0.9527, -0.5068,  0.5489,  0.5483],\n",
      "        [ 0.2265,  2.0580,  0.1450,  0.1449],\n",
      "        [-0.6741, -0.0246,  0.8244,  0.8236],\n",
      "        [-1.9953, -0.6074,  1.4858,  1.5843]])\n"
     ]
    }
   ],
   "source": [
    "cam_translation = cam_position @ torch.linalg.inv(-cam_rotation.T)\n",
    "view_transformation_matrix = getWorld2View(cam_rotation, cam_translation).T\n",
    "projection_matrix = getProjectionMatrix(znear, zfar, fov_x, fov_y).T\n",
    "full_transformation_matrix = view_transformation_matrix @ projection_matrix\n",
    "print(f\"Full projection matrix : {full_transformation_matrix}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GaussianEnv",
   "language": "python",
   "name": "24_gs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
