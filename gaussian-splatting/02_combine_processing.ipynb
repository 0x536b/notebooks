{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c09ea5-99d2-414a-9c68-b0810da799ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get all required processed data from the ply file and JSON file individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a37c882-1bf0-42f4-aeae-523a2ee73983",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files \t=/home/suresh/2024/gs/datasets/models/truck/point_cloud/iteration_30000/point_cloud.ply\n",
      "\t=/home/suresh/2024/gs/datasets/models/truck/cameras.json\n",
      "Device running on = cuda\n",
      "\n",
      "Cam center : \n",
      "tensor([ 0.4077,  0.3685, -2.2599])\n",
      "\n",
      "\n",
      "W2V matrix : \n",
      "tensor([[ 0.8014, -0.2391,  0.5483,  0.0000],\n",
      "        [ 0.1906,  0.9709,  0.1449,  0.0000],\n",
      "        [-0.5670, -0.0116,  0.8236,  0.0000],\n",
      "        [-1.6784, -0.2866,  1.5843,  1.0000]])\n",
      "\n",
      "\n",
      "Projection matrix : \n",
      "tensor([[ 1.1888,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  2.1197,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.0010,  1.0000],\n",
      "        [ 0.0000,  0.0000, -0.1001,  0.0000]])\n",
      "\n",
      "Tile size \t\t\t= 16\n",
      "Total blocks \t\t\t= 2170\n",
      "Original Width x Height \t= 1957 x 1091\n",
      "Block-adjusted Width x Height \t= 992 x 560\n",
      "Focal_x \t\t\t= 581.3302001953125\n",
      "Focal_y \t\t\t= 578.6701049804688\n",
      "FOV_x \t\t\t\t= 1.3986958265304565\n",
      "FOV_y \t\t\t\t= 0.8816215991973877\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from gs_lib import getPlyData, getJSONData, getWorld2View, getProjectionMatrix, eval_sh, \\\n",
    "    getFinalCovariance, projection, calc_radius, calc_rectangle\n",
    "\n",
    "\n",
    "# The only two files that are needed for rendering.\n",
    "dataset = f\"/home/suresh/2024/gs/datasets/models/truck\"\n",
    "iteration = \"30000\"\n",
    "max_sh_degree = 3\n",
    "json_file = f\"{dataset}/cameras.json\"\n",
    "ply_file = f\"{dataset}/point_cloud/iteration_{iteration}/point_cloud.ply\"\n",
    "view = 52\n",
    "output_scale_down = 2  # How much to scale down from the original image\n",
    "tile_size = 16  # Block size for tile-based splatting/rendering\n",
    "bg = 0  # use 1 for white, 0 for black\n",
    "\n",
    "# Print to see if proper\n",
    "print(f\"Files \\t={ply_file}\\n\\t={json_file}\")\n",
    "\n",
    "# Check for the available paraller device\n",
    "device = \"mps\" if getattr(torch, 'has_mps', False) else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device running on = {device}\")\n",
    "\n",
    "\n",
    "# Get xyz and E from plyfile\n",
    "# Get world2view_matrix and projection_matrix\n",
    "xyz, opacities, sh_all, E_raw, E = getPlyData(ply_file, max_sh_degree)\n",
    "camera_center, w, h, final_w, final_h, fov_x, fov_y, focal_x, focal_y, total_blocks, world2view_matrix, projection_matrix = getJSONData(json_file, view, output_scale_down, tile_size)\n",
    "\n",
    "# Print stuff to debug and check\n",
    "print(f\"\\nCam center : \\n{camera_center}\\n\")\n",
    "print(f\"\\nW2V matrix : \\n{world2view_matrix}\\n\")\n",
    "print(f\"\\nProjection matrix : \\n{projection_matrix}\\n\")\n",
    "print(f\"Tile size \\t\\t\\t= {tile_size}\")\n",
    "print(f\"Total blocks \\t\\t\\t= {total_blocks}\")\n",
    "print(f\"Original Width x Height \\t= {w} x {h}\")\n",
    "print(f\"Block-adjusted Width x Height \\t= {final_w} x {final_h}\")\n",
    "print(f\"Focal_x \\t\\t\\t= {focal_x}\")\n",
    "print(f\"Focal_y \\t\\t\\t= {focal_y}\")\n",
    "print(f\"FOV_x \\t\\t\\t\\t= {fov_x}\")\n",
    "print(f\"FOV_y \\t\\t\\t\\t= {fov_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e8cfd3-d17b-4160-9880-2ce17258763b",
   "metadata": {},
   "source": [
    "## Now calculate items which require data from both files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df208b2b-4d86-45e9-bf7c-d20e2120def5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Colors_precomp : \n",
      "tensor([[0.3391, 0.3510, 0.4118],\n",
      "        [0.3817, 0.3509, 0.4344],\n",
      "        [0.2098, 0.3660, 0.2177],\n",
      "        ...,\n",
      "        [1.0388, 1.0412, 1.0600],\n",
      "        [0.3586, 0.3574, 0.2428],\n",
      "        [0.8882, 0.8200, 0.9665]])\n",
      "\n",
      "\n",
      "Background : \n",
      "tensor([0., 0., 0.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Computer colors. \n",
    "# As the colors are view dependent, we need the camera center position to calculate color\n",
    "# We first calculate the distance of each gaussian from the camera\n",
    "distanceFromCam = xyz - torch.tile(camera_center, (xyz.shape[0], 1))\n",
    "distanceFromCam_normed = distanceFromCam / torch.linalg.norm(distanceFromCam, dim=1, keepdims=True)\n",
    "sh2rgb = eval_sh(max_sh_degree, sh_all, distanceFromCam_normed)\n",
    "colors_precomp = torch.clamp_min(sh2rgb + 0.5, 0.0)\n",
    "print(f\"\\nColors_precomp : \\n{colors_precomp}\\n\")\n",
    "bg_color = torch.ones([3]) if bg else torch.zeros([3])\n",
    "print(f\"\\nBackground : \\n{bg_color}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75775bb2-866a-4976-ab95-6ae8c9630794",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2541226, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Now calculate the final 2D covariance matrix.\n",
    "# This is the Equiation 5 in the official paper\n",
    "final_covariance = getFinalCovariance(\n",
    "    mean3d=xyz,\n",
    "    cov3d=E_raw,\n",
    "    viewmatrix=world2view_matrix,\n",
    "    fov_x=fov_x,\n",
    "    fov_y=fov_y,\n",
    "    focal_x=focal_x,\n",
    "    focal_y=focal_y\n",
    ")\n",
    "\n",
    "# print(f\"\\nFinal Covariance : \\n{final_covariance}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15c7bb7d-3644-4587-9146-8f202f0aa5cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now transform all the 3D gaussians to 2D screen space gaussians\n",
    "# Parameters:\n",
    "#     Projected_points : 3D-Cam-2D = world space points to camera space points to image space points)\n",
    "#     camera_space_points : 3D-Cam = world space points to camera space points.\n",
    "#     points_before_camera : culled points. This is True/False array. Points behind the camera are false.\n",
    "projected_points, camera_space_points, points_before_camera = projection(xyz, world2view_matrix, projection_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "514aa42f-87da-489d-b456-d72a106e3c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can calculate the depth from the variable camera_space_points\n",
    "# The z-axis in the camera coordinate system is the disantce from camera orgin to the world. Hence it is the depth\n",
    "depth = camera_space_points[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ea548d5-5e94-4edd-807b-e42db8f5ad26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we need to normalize all the points to NDC\n",
    "# NDC = Normalized Device Coordinate. Look it up.\n",
    "# TLDR = It brings all the points coordinate between values -1,1\n",
    "final_2d_gaussians = ((projected_points[:, :2] + 1.0) * torch.tensor([final_w, final_h]) - 1.0) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93bb7cd5-e7e1-44c2-8170-9e8d34b2a3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can remove all the unwanted gaussians that are behind the camera\n",
    "# We can use the points_before_camera binary array as a mask\n",
    "final_2d_gaussians_reduced = final_2d_gaussians[points_before_camera]\n",
    "final_covariance_reduced = final_covariance[points_before_camera]\n",
    "colors_precomp_reduced = colors_precomp[points_before_camera]\n",
    "depth_reduced = depth[points_before_camera]\n",
    "opacities_reduced = opacities[points_before_camera]\n",
    "opacities_reduced = opacities_reduced.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa2603d8-f8a8-4c55-bb66-7e2d17f99680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To splat the gaussians on the screen, we need to find the area of the gaussians\n",
    "# We first find the radius/spread of the gaussians.\n",
    "# Radius is basically the spread of the gaussian. Hence we can calculate it from the covariance matrix\n",
    "# We can calculate the top-left coordinate and bottom-right coordinate of the bounding box of the gaussian\n",
    "# We need radius, the 2d coordiante of gaussian, the width and height of the final image.\n",
    "radius = calc_radius(final_covariance_reduced)\n",
    "bounding_rectange = calc_rectangle(final_2d_gaussians_reduced, radius, final_w, final_h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GaussianEnv",
   "language": "python",
   "name": "24_gs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
